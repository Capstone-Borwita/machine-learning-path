{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Crop Bounding Box**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 416x640 1 NIK, 1 Nama, 1 Alamat, 400.9ms\n",
      "Speed: 82.8ms preprocess, 400.9ms inference, 0.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Saved: output-crop/crop_0.jpg\n",
      "Saved: output-crop/crop_1.jpg\n",
      "Saved: output-crop/crop_2.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[[231, 193, 158],\n",
       "         [235, 195, 160],\n",
       "         [234, 194, 159],\n",
       "         ...,\n",
       "         [227, 190, 156],\n",
       "         [225, 188, 154],\n",
       "         [225, 189, 153]],\n",
       " \n",
       "        [[219, 181, 147],\n",
       "         [223, 185, 151],\n",
       "         [227, 189, 155],\n",
       "         ...,\n",
       "         [224, 188, 152],\n",
       "         [223, 189, 153],\n",
       "         [221, 188, 149]],\n",
       " \n",
       "        [[236, 198, 164],\n",
       "         [238, 200, 166],\n",
       "         [239, 201, 167],\n",
       "         ...,\n",
       "         [220, 184, 148],\n",
       "         [224, 191, 152],\n",
       "         [223, 190, 151]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[241, 202, 164],\n",
       "         [244, 205, 167],\n",
       "         [243, 204, 166],\n",
       "         ...,\n",
       "         [239, 200, 168],\n",
       "         [237, 199, 167],\n",
       "         [234, 196, 164]],\n",
       " \n",
       "        [[231, 194, 156],\n",
       "         [235, 196, 158],\n",
       "         [233, 194, 156],\n",
       "         ...,\n",
       "         [240, 202, 172],\n",
       "         [240, 202, 172],\n",
       "         [237, 199, 169]],\n",
       " \n",
       "        [[233, 196, 158],\n",
       "         [236, 197, 159],\n",
       "         [234, 195, 157],\n",
       "         ...,\n",
       "         [233, 195, 165],\n",
       "         [231, 193, 163],\n",
       "         [227, 191, 161]]], dtype=uint8),\n",
       " array([[[230, 188, 146],\n",
       "         [230, 188, 146],\n",
       "         [231, 189, 146],\n",
       "         ...,\n",
       "         [230, 193, 167],\n",
       "         [231, 194, 168],\n",
       "         [232, 195, 169]],\n",
       " \n",
       "        [[228, 186, 144],\n",
       "         [227, 185, 142],\n",
       "         [228, 186, 143],\n",
       "         ...,\n",
       "         [235, 198, 172],\n",
       "         [235, 198, 172],\n",
       "         [236, 199, 173]],\n",
       " \n",
       "        [[223, 183, 141],\n",
       "         [225, 183, 140],\n",
       "         [225, 184, 139],\n",
       "         ...,\n",
       "         [235, 199, 175],\n",
       "         [236, 200, 176],\n",
       "         [236, 200, 176]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[225, 187, 153],\n",
       "         [228, 189, 157],\n",
       "         [231, 193, 163],\n",
       "         ...,\n",
       "         [233, 200, 174],\n",
       "         [232, 199, 173],\n",
       "         [233, 200, 174]],\n",
       " \n",
       "        [[235, 197, 163],\n",
       "         [236, 197, 165],\n",
       "         [231, 193, 163],\n",
       "         ...,\n",
       "         [230, 200, 171],\n",
       "         [229, 199, 170],\n",
       "         [230, 200, 171]],\n",
       " \n",
       "        [[240, 202, 168],\n",
       "         [240, 201, 169],\n",
       "         [233, 195, 165],\n",
       "         ...,\n",
       "         [226, 196, 167],\n",
       "         [225, 195, 166],\n",
       "         [227, 197, 168]]], dtype=uint8),\n",
       " array([[[231, 195, 165],\n",
       "         [233, 195, 165],\n",
       "         [232, 194, 164],\n",
       "         ...,\n",
       "         [214, 180, 144],\n",
       "         [200, 165, 131],\n",
       "         [204, 169, 135]],\n",
       " \n",
       "        [[233, 196, 168],\n",
       "         [223, 186, 158],\n",
       "         [223, 187, 157],\n",
       "         ...,\n",
       "         [224, 190, 154],\n",
       "         [222, 187, 153],\n",
       "         [227, 192, 158]],\n",
       " \n",
       "        [[231, 196, 170],\n",
       "         [235, 200, 174],\n",
       "         [227, 193, 164],\n",
       "         ...,\n",
       "         [227, 193, 157],\n",
       "         [235, 198, 164],\n",
       "         [239, 202, 168]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[225, 182, 149],\n",
       "         [223, 182, 149],\n",
       "         [224, 183, 151],\n",
       "         ...,\n",
       "         [236, 200, 176],\n",
       "         [236, 201, 175],\n",
       "         [235, 200, 174]],\n",
       " \n",
       "        [[231, 189, 154],\n",
       "         [225, 185, 150],\n",
       "         [219, 178, 145],\n",
       "         ...,\n",
       "         [237, 201, 177],\n",
       "         [237, 202, 176],\n",
       "         [236, 201, 175]],\n",
       " \n",
       "        [[229, 187, 150],\n",
       "         [227, 188, 150],\n",
       "         [215, 175, 140],\n",
       "         ...,\n",
       "         [234, 198, 174],\n",
       "         [234, 199, 173],\n",
       "         [234, 199, 173]]], dtype=uint8)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize YOLO model\n",
    "model = YOLO(\"D:/Bangkit Capstone/Object-Detection-Bounding-Box/Object_Detection_Bounding_Box/Train result/train/weights/Model_Detection_Label.pt\")\n",
    "\n",
    "# Detect objects in an image\n",
    "def detect_and_crop(image_path, output_folder, confidence_threshold=0.5, save_crops=False):\n",
    "    # Check if output folder exists, create if not\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Unable to read image {image_path}\")\n",
    "        return []\n",
    "\n",
    "    results = model(img)\n",
    "    boxes = results[0].boxes  \n",
    "\n",
    "    cropped_images = []\n",
    "    for i, box in enumerate(boxes):\n",
    "        confidence = box.conf.item()\n",
    "        if confidence < confidence_threshold:\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        cropped = img[y1:y2, x1:x2]\n",
    "        cropped_images.append(cropped)\n",
    "\n",
    "        # Save cropped regions (optional)\n",
    "        if save_crops:\n",
    "            output_path = f\"{output_folder}/crop_{i}.jpg\"\n",
    "            cv2.imwrite(output_path, cropped)\n",
    "            print(f\"Saved: {output_path}\")\n",
    "    return cropped_images\n",
    "\n",
    "detect_and_crop(\"D:/Bangkit Capstone/Object-Detection-Bounding-Box/Object_Detection_Bounding_Box/inputs-ktp/test-ktp.jpg\", \"output-crop\", confidence_threshold=0.7, save_crops=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with confidence threshold: 0.4\n",
      "\n",
      "0: 480x640 1 NIK, 1 Nama, 1 Alamat, 259.5ms\n",
      "Speed: 0.0ms preprocess, 259.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved: output-test_threshold_0.4/crop_0.jpg\n",
      "Saved: output-test_threshold_0.4/crop_1.jpg\n",
      "Saved: output-test_threshold_0.4/crop_2.jpg\n",
      "\n",
      "Testing with confidence threshold: 0.5\n",
      "\n",
      "0: 480x640 1 NIK, 1 Nama, 1 Alamat, 211.6ms\n",
      "Speed: 0.0ms preprocess, 211.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved: output-test_threshold_0.5/crop_0.jpg\n",
      "Saved: output-test_threshold_0.5/crop_1.jpg\n",
      "Saved: output-test_threshold_0.5/crop_2.jpg\n",
      "\n",
      "Testing with confidence threshold: 0.6\n",
      "\n",
      "0: 480x640 1 NIK, 1 Nama, 1 Alamat, 226.6ms\n",
      "Speed: 8.0ms preprocess, 226.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved: output-test_threshold_0.6/crop_0.jpg\n",
      "Saved: output-test_threshold_0.6/crop_1.jpg\n",
      "Saved: output-test_threshold_0.6/crop_2.jpg\n",
      "\n",
      "Testing with confidence threshold: 0.7\n",
      "\n",
      "0: 480x640 1 NIK, 1 Nama, 1 Alamat, 280.8ms\n",
      "Speed: 0.0ms preprocess, 280.8ms inference, 8.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved: output-test_threshold_0.7/crop_0.jpg\n",
      "Saved: output-test_threshold_0.7/crop_1.jpg\n",
      "Saved: output-test_threshold_0.7/crop_2.jpg\n",
      "\n",
      "Testing with confidence threshold: 0.8\n",
      "\n",
      "0: 480x640 1 NIK, 1 Nama, 1 Alamat, 184.1ms\n",
      "Speed: 3.5ms preprocess, 184.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved: output-test_threshold_0.8/crop_0.jpg\n"
     ]
    }
   ],
   "source": [
    "# def test_thresholds(image_path, output_folder, thresholds):\n",
    "#     for threshold in thresholds:\n",
    "#         print(f\"\\nTesting with confidence threshold: {threshold}\")\n",
    "#         detect_and_crop(image_path, f\"{output_folder}_threshold_{threshold}\", confidence_threshold=threshold, save_crops=True)\n",
    "\n",
    "# # Example usage\n",
    "# threshold_values = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "# test_thresholds(\"image.png\", \"output-test\", threshold_values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
